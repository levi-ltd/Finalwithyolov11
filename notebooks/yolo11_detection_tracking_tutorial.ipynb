{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60cbf739",
   "metadata": {},
   "source": [
    "# YOLOv11 Object Detection and Tracking Tutorial\n",
    "\n",
    "This comprehensive notebook demonstrates how to build a complete object detection and tracking system using YOLOv11 with Label Studio for data annotation.\n",
    "\n",
    "## Features Covered:\n",
    "- üéØ Custom dataset preparation with Label Studio\n",
    "- ü§ñ YOLO 11 model training and fine-tuning\n",
    "- üìπ Real-time camera object detection\n",
    "- üîÑ Multi-object tracking with unique IDs\n",
    "- üíæ Export tracking results and annotated videos\n",
    "\n",
    "## Prerequisites:\n",
    "- Python 3.8+\n",
    "- Webcam or video file for testing\n",
    "- Label Studio account (optional for advanced features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c866a9",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies\n",
    "\n",
    "First, let's install all the necessary packages for YOLO 11, Label Studio, and object tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da284943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Install core packages for YOLO 11 and computer vision\n",
    "!pip install ultralytics>=8.0.196\n",
    "!pip install opencv-python>=4.8.0\n",
    "!pip install torch torchvision\n",
    "\n",
    "# Install Label Studio and SDK for dataset management\n",
    "!pip install label-studio>=1.9.0\n",
    "!pip install label-studio-sdk>=0.0.31\n",
    "\n",
    "# Install tracking and data processing libraries\n",
    "!pip install filterpy>=1.4.5\n",
    "!pip install scipy>=1.10.0\n",
    "!pip install numpy>=1.24.0\n",
    "!pip install pandas>=2.0.0\n",
    "!pip install matplotlib>=3.7.0\n",
    "\n",
    "# Additional utilities\n",
    "!pip install tqdm pyyaml requests\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4b334",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup\n",
    "\n",
    "Import all necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "# YOLO 11 and deep learning\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Label Studio SDK\n",
    "from label_studio_sdk import Client\n",
    "\n",
    "# Tracking libraries\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Utility libraries\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib for notebook display\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = ['data', 'models', 'results', 'videos']\n",
    "for dir_name in output_dirs:\n",
    "    Path(dir_name).mkdir(exist_ok=True)\n",
    "    \n",
    "print(\"üìÅ Output directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944ba96",
   "metadata": {},
   "source": [
    "## 3. Setup Label Studio Connection\n",
    "\n",
    "Configure the connection to Label Studio for dataset management and annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e0551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Studio configuration\n",
    "LABEL_STUDIO_URL = \"http://localhost:8080\"\n",
    "LABEL_STUDIO_API_TOKEN = \"\"  # Add your API token here\n",
    "PROJECT_NAME = \"YOLOv11_Simple_Detection_Dataset\"\n",
    "\n",
    "class LabelStudioManager:\n",
    "    def __init__(self, url, api_token=None):\n",
    "        self.url = url\n",
    "        self.api_token = api_token\n",
    "        self.client = None\n",
    "        self.project = None\n",
    "        \n",
    "        if api_token:\n",
    "            try:\n",
    "                self.client = Client(url=url, api_key=api_token)\n",
    "                print(\"‚úÖ Connected to Label Studio\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to connect to Label Studio: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No API token provided. Some features will be limited.\")\n",
    "    \n",
    "    def create_project(self, project_name, label_config=None):\n",
    "        if not self.client:\n",
    "            print(\"‚ùå No Label Studio client available\")\n",
    "            return None\n",
    "        \n",
    "        if not label_config:\n",
    "            # Simple 3-class object detection label config\n",
    "            label_config = '''\n",
    "            <View>\n",
    "              <Image name=\"image\" value=\"$image\"/>\n",
    "              <RectangleLabels name=\"label\" toName=\"image\">\n",
    "                <Label value=\"person\" background=\"red\"/>\n",
    "                <Label value=\"micro\" background=\"blue\"/>\n",
    "                <Label value=\"singer\" background=\"green\"/>\n",
    "              </RectangleLabels>\n",
    "            </View>\n",
    "            '''\n",
    "        \n",
    "        try:\n",
    "            self.project = self.client.start_project(\n",
    "                title=project_name,\n",
    "                label_config=label_config,\n",
    "                description=\"Simplified object detection dataset for person, microphone, and singer tracking\"\n",
    "            )\n",
    "            print(f\"‚úÖ Created project: {project_name}\")\n",
    "            return self.project\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create project: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Label Studio manager\n",
    "ls_manager = LabelStudioManager(LABEL_STUDIO_URL, LABEL_STUDIO_API_TOKEN)\n",
    "\n",
    "# If you don't have Label Studio running, you can skip this section\n",
    "# and work with sample data\n",
    "print(\"üìù Label Studio setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef88eb",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Dataset from Label Studio\n",
    "\n",
    "Fetch labeled data from Label Studio and convert annotations to YOLO format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd615fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labelstudio_to_yolo(annotations, class_mapping):\n",
    "    \"\"\"Convert Label Studio annotations to YOLO format\"\"\"\n",
    "    yolo_annotations = []\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        for result in annotation.get('result', []):\n",
    "            if result['type'] == 'rectanglelabels':\n",
    "                value = result['value']\n",
    "                class_name = value['rectanglelabels'][0]\n",
    "                \n",
    "                if class_name not in class_mapping:\n",
    "                    continue\n",
    "                \n",
    "                class_id = class_mapping[class_name]\n",
    "                \n",
    "                # Convert Label Studio coordinates to YOLO format\n",
    "                # Label Studio uses percentages, YOLO needs normalized center coordinates\n",
    "                x = value['x'] / 100.0\n",
    "                y = value['y'] / 100.0\n",
    "                width = value['width'] / 100.0\n",
    "                height = value['height'] / 100.0\n",
    "                \n",
    "                # Convert to center coordinates\n",
    "                x_center = x + width / 2\n",
    "                y_center = y + height / 2\n",
    "                \n",
    "                yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "    \n",
    "    return yolo_annotations\n",
    "\n",
    "def prepare_yolo_dataset(output_dir=\"data/yolo_dataset\"):\n",
    "    \"\"\"Prepare dataset in YOLO format\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        (output_path / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
    "        (output_path / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Define simplified class mapping for person, micro, and singer\n",
    "    class_mapping = {\n",
    "        'person': 0,\n",
    "        'micro': 1,\n",
    "        'singer': 2\n",
    "    }\n",
    "    \n",
    "    # Create dataset.yaml file\n",
    "    dataset_config = {\n",
    "        'path': str(output_path.absolute()),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': 'images/test',\n",
    "        'nc': len(class_mapping),\n",
    "        'names': list(class_mapping.keys())\n",
    "    }\n",
    "    \n",
    "    with open(output_path / 'dataset.yaml', 'w') as f:\n",
    "        yaml.dump(dataset_config, f)\n",
    "    \n",
    "    print(f\"‚úÖ YOLO dataset structure created at: {output_path}\")\n",
    "    print(f\"üìä Classes: {list(class_mapping.keys())}\")\n",
    "    \n",
    "    return str(output_path / 'dataset.yaml'), class_mapping\n",
    "\n",
    "# Create sample dataset structure (for demo purposes)\n",
    "dataset_config_path, class_mapping = prepare_yolo_dataset()\n",
    "print(f\"Dataset config saved to: {dataset_config_path}\")\n",
    "\n",
    "# If you have Label Studio data, you can export and convert it here\n",
    "# For now, we'll proceed with the pretrained YOLO model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933b0d7",
   "metadata": {},
   "source": [
    "## 5. Configure YOLO 11 Model\n",
    "\n",
    "Load the YOLO 11 model and configure it for our detection and tracking system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63325004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO model configuration\n",
    "MODEL_SIZE = \"yolo11n\"  # Options: yolo11n, yolo11s, yolo11m, yolo11l, yolo11x\n",
    "CONFIDENCE_THRESHOLD = 0.25\n",
    "IOU_THRESHOLD = 0.45\n",
    "\n",
    "class YOLODetector:\n",
    "    def __init__(self, model_name=MODEL_SIZE, conf_thresh=CONFIDENCE_THRESHOLD, iou_thresh=IOU_THRESHOLD):\n",
    "        \"\"\"Initialize YOLO detector\"\"\"\n",
    "        print(f\"üîÑ Loading YOLO 11 model: {model_name}\")\n",
    "        self.model = YOLO(f\"{model_name}.pt\")\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.iou_thresh = iou_thresh\n",
    "        \n",
    "        # Check device\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"üñ•Ô∏è Using device: {self.device}\")\n",
    "        \n",
    "        # Get class names\n",
    "        self.class_names = self.model.names\n",
    "        print(f\"üìã Model classes: {len(self.class_names)} classes\")\n",
    "        \n",
    "    def detect(self, frame, verbose=False):\n",
    "        \"\"\"Run detection on a frame\"\"\"\n",
    "        results = self.model(\n",
    "            frame,\n",
    "            conf=self.conf_thresh,\n",
    "            iou=self.iou_thresh,\n",
    "            verbose=verbose,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        detections = []\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    # Extract detection data\n",
    "                    xyxy = box.xyxy[0].cpu().numpy()\n",
    "                    conf = float(box.conf[0])\n",
    "                    class_id = int(box.cls[0])\n",
    "                    class_name = self.class_names[class_id]\n",
    "                    \n",
    "                    detection = {\n",
    "                        'bbox': xyxy,\n",
    "                        'confidence': conf,\n",
    "                        'class_id': class_id,\n",
    "                        'class_name': class_name\n",
    "                    }\n",
    "                    detections.append(detection)\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def draw_detections(self, frame, detections, draw_conf=True):\n",
    "        \"\"\"Draw detection boxes on frame\"\"\"\n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        for detection in detections:\n",
    "            bbox = detection['bbox']\n",
    "            conf = detection['confidence']\n",
    "            class_name = detection['class_name']\n",
    "            \n",
    "            # Draw bounding box\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            color = (0, 255, 0)  # Green\n",
    "            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Draw label\n",
    "            label = f\"{class_name}\"\n",
    "            if draw_conf:\n",
    "                label += f\": {conf:.2f}\"\n",
    "            \n",
    "            label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "            cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 10), \n",
    "                         (x1 + label_size[0], y1), color, -1)\n",
    "            cv2.putText(annotated_frame, label, (x1, y1 - 5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        return annotated_frame\n",
    "\n",
    "# Initialize YOLO detector\n",
    "detector = YOLODetector()\n",
    "\n",
    "print(\"‚úÖ YOLO 11 detector initialized!\")\n",
    "print(f\"Available classes: {list(detector.class_names.values())[:10]}...\")  # Show first 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc9c672",
   "metadata": {},
   "source": [
    "## 6. Train Custom YOLO 11 Model (Optional)\n",
    "\n",
    "If you have custom labeled data, you can fine-tune the YOLO 11 model on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration (uncomment to train a custom model)\n",
    "TRAIN_CUSTOM_MODEL = False  # Set to True if you want to train on your data\n",
    "\n",
    "def train_custom_yolo(dataset_config_path, epochs=100, batch_size=16, img_size=640):\n",
    "    \"\"\"Train a custom YOLO 11 model\"\"\"\n",
    "    print(f\"üèãÔ∏è Starting YOLO 11 training...\")\n",
    "    print(f\"Dataset: {dataset_config_path}\")\n",
    "    print(f\"Epochs: {epochs}, Batch size: {batch_size}, Image size: {img_size}\")\n",
    "    \n",
    "    # Initialize model for training\n",
    "    model = YOLO(\"yolo11n.pt\")  # Start with pretrained weights\n",
    "    \n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=dataset_config_path,\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=img_size,\n",
    "        project=\"models\",\n",
    "        name=\"custom_yolo11\",\n",
    "        exist_ok=True,\n",
    "        verbose=True,\n",
    "        save_period=10  # Save checkpoint every 10 epochs\n",
    "    )\n",
    "    \n",
    "    # Return path to best model\n",
    "    best_model_path = results.save_dir / \"weights\" / \"best.pt\"\n",
    "    print(f\"‚úÖ Training completed! Best model: {best_model_path}\")\n",
    "    \n",
    "    return str(best_model_path)\n",
    "\n",
    "def validate_model(model_path, dataset_config_path):\n",
    "    \"\"\"Validate the trained model\"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Run validation\n",
    "    results = model.val(data=dataset_config_path, verbose=True)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"üìä Validation Results:\")\n",
    "    print(f\"mAP50: {results.box.map50:.4f}\")\n",
    "    print(f\"mAP50-95: {results.box.map:.4f}\")\n",
    "    print(f\"Precision: {results.box.mp:.4f}\")\n",
    "    print(f\"Recall: {results.box.mr:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if TRAIN_CUSTOM_MODEL:\n",
    "    # Train custom model (only if you have labeled data)\n",
    "    print(\"üéØ Training custom YOLO 11 model...\")\n",
    "    custom_model_path = train_custom_yolo(\n",
    "        dataset_config_path,\n",
    "        epochs=50,  # Reduced for demo\n",
    "        batch_size=8,\n",
    "        img_size=640\n",
    "    )\n",
    "    \n",
    "    # Validate the model\n",
    "    validation_results = validate_model(custom_model_path, dataset_config_path)\n",
    "    \n",
    "    # Update detector with custom model\n",
    "    detector = YOLODetector(custom_model_path)\n",
    "    print(\"‚úÖ Custom model loaded for detection!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Skipping custom training - using pretrained YOLO 11 model\")\n",
    "    print(\"üí° Set TRAIN_CUSTOM_MODEL = True to train on your custom dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a133e",
   "metadata": {},
   "source": [
    "## 7. Initialize Camera and Tracking System\n",
    "\n",
    "Set up camera capture and initialize the multi-object tracking system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ca80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking configuration\n",
    "MAX_TRACK_AGE = 30\n",
    "MIN_TRACK_HITS = 3\n",
    "IOU_THRESHOLD = 0.3\n",
    "PROXIMITY_THRESHOLD = 50  # pixels for person-micro proximity detection\n",
    "\n",
    "class SimpleTracker:\n",
    "    \"\"\"Simple multi-object tracker with singer detection logic\"\"\"\n",
    "    \n",
    "    def __init__(self, max_age=MAX_TRACK_AGE, min_hits=MIN_TRACK_HITS, iou_threshold=IOU_THRESHOLD):\n",
    "        self.max_age = max_age\n",
    "        self.min_hits = min_hits\n",
    "        self.iou_threshold = iou_threshold\n",
    "        \n",
    "        self.tracks = []\n",
    "        self.track_id_counter = 0\n",
    "        \n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"Calculate Intersection over Union (IoU) between two boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = (x2 - x1) * (y2 - y1)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def calculate_distance(self, box1, box2):\n",
    "        \"\"\"Calculate distance between centers of two boxes\"\"\"\n",
    "        center1_x = (box1[0] + box1[2]) / 2\n",
    "        center1_y = (box1[1] + box1[3]) / 2\n",
    "        center2_x = (box2[0] + box2[2]) / 2\n",
    "        center2_y = (box2[1] + box2[3]) / 2\n",
    "        \n",
    "        return ((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2) ** 0.5\n",
    "    \n",
    "    def detect_singers(self, detections):\n",
    "        \"\"\"Detect singers based on person-microphone proximity\"\"\"\n",
    "        modified_detections = []\n",
    "        person_indices = []\n",
    "        micro_indices = []\n",
    "        \n",
    "        # Separate persons and microphones\n",
    "        for i, detection in enumerate(detections):\n",
    "            if detection['class_name'] == 'person':\n",
    "                person_indices.append(i)\n",
    "            elif detection['class_name'] == 'micro':\n",
    "                micro_indices.append(i)\n",
    "            else:\n",
    "                modified_detections.append(detection)\n",
    "        \n",
    "        # Check each person for nearby microphone\n",
    "        used_micros = set()\n",
    "        for person_idx in person_indices:\n",
    "            person_detection = detections[person_idx]\n",
    "            closest_micro = None\n",
    "            min_distance = float('inf')\n",
    "            closest_micro_idx = None\n",
    "            \n",
    "            # Find closest microphone\n",
    "            for micro_idx in micro_indices:\n",
    "                if micro_idx in used_micros:\n",
    "                    continue\n",
    "                    \n",
    "                micro_detection = detections[micro_idx]\n",
    "                distance = self.calculate_distance(person_detection['bbox'], micro_detection['bbox'])\n",
    "                \n",
    "                if distance < min_distance and distance < PROXIMITY_THRESHOLD:\n",
    "                    min_distance = distance\n",
    "                    closest_micro = micro_detection\n",
    "                    closest_micro_idx = micro_idx\n",
    "            \n",
    "            if closest_micro is not None:\n",
    "                # Convert person with microphone to singer\n",
    "                singer_detection = person_detection.copy()\n",
    "                singer_detection['class_name'] = 'singer'\n",
    "                singer_detection['class_id'] = 2  # Singer class ID\n",
    "                singer_detection['original_class'] = 'person'\n",
    "                singer_detection['has_micro'] = True\n",
    "                singer_detection['micro_distance'] = min_distance\n",
    "                \n",
    "                modified_detections.append(singer_detection)\n",
    "                used_micros.add(closest_micro_idx)\n",
    "                \n",
    "                # Still add the microphone as separate detection\n",
    "                modified_detections.append(closest_micro)\n",
    "            else:\n",
    "                # Add person without microphone\n",
    "                modified_detections.append(person_detection)\n",
    "        \n",
    "        # Add remaining unused microphones\n",
    "        for micro_idx in micro_indices:\n",
    "            if micro_idx not in used_micros:\n",
    "                modified_detections.append(detections[micro_idx])\n",
    "        \n",
    "        return modified_detections\n",
    "    \n",
    "    def update(self, detections):\n",
    "        \"\"\"Update tracks with new detections\"\"\"\n",
    "        # First, detect singers based on person-microphone proximity\n",
    "        detections = self.detect_singers(detections)\n",
    "        \n",
    "        # Predict existing tracks\n",
    "        for track in self.tracks:\n",
    "            track['age'] += 1\n",
    "            track['time_since_update'] += 1\n",
    "        \n",
    "        # Associate detections with tracks\n",
    "        matched_tracks = []\n",
    "        unmatched_detections = list(range(len(detections)))\n",
    "        \n",
    "        for i, track in enumerate(self.tracks):\n",
    "            if track['time_since_update'] > self.max_age:\n",
    "                continue\n",
    "                \n",
    "            best_match = -1\n",
    "            best_iou = 0\n",
    "            \n",
    "            for j in unmatched_detections:\n",
    "                iou = self.calculate_iou(track['bbox'], detections[j]['bbox'])\n",
    "                if iou > best_iou and iou > self.iou_threshold:\n",
    "                    best_iou = iou\n",
    "                    best_match = j\n",
    "            \n",
    "            if best_match != -1:\n",
    "                # Update track with matched detection\n",
    "                track['bbox'] = detections[best_match]['bbox']\n",
    "                track['confidence'] = detections[best_match]['confidence']\n",
    "                track['class_name'] = detections[best_match]['class_name']\n",
    "                track['hits'] += 1\n",
    "                track['time_since_update'] = 0\n",
    "                \n",
    "                # Store additional singer info if available\n",
    "                if 'has_micro' in detections[best_match]:\n",
    "                    track['has_micro'] = detections[best_match]['has_micro']\n",
    "                    track['micro_distance'] = detections[best_match].get('micro_distance', 0)\n",
    "                \n",
    "                matched_tracks.append(i)\n",
    "                unmatched_detections.remove(best_match)\n",
    "        \n",
    "        # Create new tracks for unmatched detections\n",
    "        for j in unmatched_detections:\n",
    "            new_track = {\n",
    "                'id': self.track_id_counter,\n",
    "                'bbox': detections[j]['bbox'],\n",
    "                'confidence': detections[j]['confidence'],\n",
    "                'class_name': detections[j]['class_name'],\n",
    "                'class_id': detections[j]['class_id'],\n",
    "                'hits': 1,\n",
    "                'age': 0,\n",
    "                'time_since_update': 0\n",
    "            }\n",
    "            \n",
    "            # Add singer-specific info if available\n",
    "            if 'has_micro' in detections[j]:\n",
    "                new_track['has_micro'] = detections[j]['has_micro']\n",
    "                new_track['micro_distance'] = detections[j].get('micro_distance', 0)\n",
    "                new_track['original_class'] = detections[j].get('original_class', 'person')\n",
    "            \n",
    "            self.tracks.append(new_track)\n",
    "            self.track_id_counter += 1\n",
    "        \n",
    "        # Remove old tracks\n",
    "        self.tracks = [track for track in self.tracks \n",
    "                      if track['time_since_update'] <= self.max_age]\n",
    "        \n",
    "        # Return active tracks\n",
    "        active_tracks = [track for track in self.tracks\n",
    "                        if track['hits'] >= self.min_hits and track['time_since_update'] == 0]\n",
    "        \n",
    "        return active_tracks\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = SimpleTracker()\n",
    "print(\"‚úÖ Object tracker initialized with singer detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16fb147",
   "metadata": {},
   "source": [
    "## 8. Implement Object Detection and Tracking Functions\n",
    "\n",
    "Create the core functions for object detection and tracking integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tracked_objects(frame, tracked_objects, show_trails=False):\n",
    "    \"\"\"Draw tracked objects with IDs on frame\"\"\"\n",
    "    annotated_frame = frame.copy()\n",
    "    \n",
    "    # Colors for specific classes: person=red, micro=blue, singer=green\n",
    "    class_colors = {'person': (0, 0, 255), 'micro': (255, 0, 0), 'singer': (0, 255, 0)}\n",
    "    # Fallback colors for different track IDs\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), \n",
    "              (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 165, 0)]\n",
    "    \n",
    "    for track in tracked_objects:\n",
    "        bbox = track['bbox']\n",
    "        track_id = track['id']\n",
    "        class_name = track['class_name']\n",
    "        confidence = track['confidence']\n",
    "        \n",
    "        # Get color based on class first, then track ID\n",
    "        color = class_colors.get(class_name, colors[track_id % len(colors)])\n",
    "        \n",
    "        # Draw bounding box\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "        \n",
    "        # Draw label with track ID and additional info for singers\n",
    "        label = f\"ID:{track_id} {class_name}: {confidence:.2f}\"\n",
    "        if class_name == 'singer' and track.get('has_micro', False):\n",
    "            micro_dist = track.get('micro_distance', 0)\n",
    "            label += f\" (üé§ {micro_dist:.0f}px)\"\n",
    "        \n",
    "        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "        \n",
    "        # Background for label\n",
    "        cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 10), \n",
    "                     (x1 + label_size[0], y1), color, -1)\n",
    "        \n",
    "        # Text\n",
    "        cv2.putText(annotated_frame, label, (x1, y1 - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        \n",
    "        # Draw center point\n",
    "        center_x = int((x1 + x2) / 2)\n",
    "        center_y = int((y1 + y2) / 2)\n",
    "        cv2.circle(annotated_frame, (center_x, center_y), 4, color, -1)\n",
    "        \n",
    "        # Special indicator for singers\n",
    "        if class_name == 'singer':\n",
    "            # Draw microphone icon indicator\n",
    "            cv2.putText(annotated_frame, \"üé§\", (x2 - 20, y1 + 15), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    \n",
    "    return annotated_frame\n",
    "\n",
    "def draw_info_panel(frame, detections_count, tracks_count, fps):\n",
    "    \"\"\"Draw information panel on frame\"\"\"\n",
    "    annotated_frame = frame.copy()\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    # Panel background\n",
    "    panel_height = 120\n",
    "    panel_width = 300\n",
    "    overlay = annotated_frame.copy()\n",
    "    cv2.rectangle(overlay, (10, 10), (10 + panel_width, 10 + panel_height), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.7, annotated_frame, 0.3, 0, annotated_frame)\n",
    "    \n",
    "    # Panel border\n",
    "    cv2.rectangle(annotated_frame, (10, 10), (10 + panel_width, 10 + panel_height), (255, 255, 255), 2)\n",
    "    \n",
    "    # Information text\n",
    "    info_text = [\n",
    "        f\"FPS: {fps:.1f}\",\n",
    "        f\"Detections: {detections_count}\",\n",
    "        f\"Active Tracks: {tracks_count}\",\n",
    "        f\"Time: {datetime.now().strftime('%H:%M:%S')}\"\n",
    "    ]\n",
    "    \n",
    "    y_offset = 35\n",
    "    for text in info_text:\n",
    "        cv2.putText(annotated_frame, text, (20, y_offset), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        y_offset += 25\n",
    "    \n",
    "    return annotated_frame\n",
    "\n",
    "def process_frame(frame, detector, tracker, draw_info=True):\n",
    "    \"\"\"Process a single frame through detection and tracking pipeline\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run object detection\n",
    "    detections = detector.detect(frame)\n",
    "    \n",
    "    # Filter detections to only include our 3 classes (person, micro, singer)\n",
    "    filtered_detections = []\n",
    "    for detection in detections:\n",
    "        class_name = detection['class_name']\n",
    "        # Map YOLO classes to our simplified classes\n",
    "        if class_name in ['person', 'microphone', 'mic']:\n",
    "            if class_name in ['microphone', 'mic']:\n",
    "                detection['class_name'] = 'micro'\n",
    "                detection['class_id'] = 1\n",
    "            filtered_detections.append(detection)\n",
    "    \n",
    "    # Update tracker with filtered detections\n",
    "    tracked_objects = tracker.update(filtered_detections)\n",
    "    \n",
    "    # Draw results\n",
    "    annotated_frame = draw_tracked_objects(frame, tracked_objects)\n",
    "    \n",
    "    # Calculate FPS\n",
    "    processing_time = time.time() - start_time\n",
    "    fps = 1.0 / processing_time if processing_time > 0 else 0\n",
    "    \n",
    "    # Draw info panel\n",
    "    if draw_info:\n",
    "        annotated_frame = draw_info_panel(\n",
    "            annotated_frame, \n",
    "            len(filtered_detections), \n",
    "            len(tracked_objects), \n",
    "            fps\n",
    "        )\n",
    "    \n",
    "    return annotated_frame, tracked_objects, filtered_detections, fps\n",
    "\n",
    "def test_detection_on_sample():\n",
    "    \"\"\"Test detection on a sample image with simulated objects\"\"\"\n",
    "    # Create a sample image\n",
    "    test_image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    cv2.putText(test_image, \"Simplified Detection: Person + Micro = Singer\", \n",
    "                (50, 240), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    # Create fake detections to test the singer logic\n",
    "    fake_detections = [\n",
    "        {\n",
    "            'bbox': np.array([100, 150, 200, 350]),  # Person\n",
    "            'confidence': 0.85,\n",
    "            'class_name': 'person',\n",
    "            'class_id': 0\n",
    "        },\n",
    "        {\n",
    "            'bbox': np.array([180, 200, 220, 240]),  # Microphone near person\n",
    "            'confidence': 0.90,\n",
    "            'class_name': 'micro',\n",
    "            'class_id': 1\n",
    "        },\n",
    "        {\n",
    "            'bbox': np.array([400, 100, 500, 300]),  # Another person\n",
    "            'confidence': 0.75,\n",
    "            'class_name': 'person',\n",
    "            'class_id': 0\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Test singer detection\n",
    "    tracked_objects = tracker.update(fake_detections)\n",
    "    annotated_frame = draw_tracked_objects(test_image, tracked_objects)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Original Frame\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Processed Frame (Singer Detection)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Detection Results:\")\n",
    "    print(f\"   Original detections: {len(fake_detections)}\")\n",
    "    print(f\"   Active tracks: {len(tracked_objects)}\")\n",
    "    for track in tracked_objects:\n",
    "        class_info = track['class_name']\n",
    "        if track.get('has_micro'):\n",
    "            class_info += f\" (with microphone, distance: {track.get('micro_distance', 0):.0f}px)\"\n",
    "        print(f\"   Track {track['id']}: {class_info}\")\n",
    "\n",
    "# Test the detection pipeline\n",
    "print(\"üß™ Testing simplified detection pipeline...\")\n",
    "test_detection_on_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3b0a3",
   "metadata": {},
   "source": [
    "## 9. Real-time Camera Processing\n",
    "\n",
    "Run the complete detection and tracking system on live camera feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c922e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera processing configuration\n",
    "RUN_CAMERA_DEMO = False  # Set to True to run camera demo\n",
    "DEMO_DURATION = 30  # seconds\n",
    "SAVE_VIDEO = True\n",
    "\n",
    "# Simple camera manager class\n",
    "class CameraManager:\n",
    "    def __init__(self):\n",
    "        self.cap = None\n",
    "        \n",
    "    def start_camera(self, camera_id=0):\n",
    "        \"\"\"Start camera capture\"\"\"\n",
    "        self.cap = cv2.VideoCapture(camera_id)\n",
    "        if self.cap.isOpened():\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_frame(self):\n",
    "        \"\"\"Get current frame from camera\"\"\"\n",
    "        if self.cap is None:\n",
    "            return None\n",
    "        ret, frame = self.cap.read()\n",
    "        return frame if ret else None\n",
    "    \n",
    "    def release(self):\n",
    "        \"\"\"Release camera resources\"\"\"\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "# Initialize camera manager\n",
    "camera_manager = CameraManager()\n",
    "\n",
    "def run_camera_detection(duration=30, save_video=False, video_path=\"results/detection_output.mp4\"):\n",
    "    \"\"\"Run real-time object detection and tracking on camera feed\"\"\"\n",
    "    try:\n",
    "        # Initialize camera\n",
    "        print(\"üìπ Starting camera...\")\n",
    "        if not camera_manager.start_camera():\n",
    "            print(\"‚ùå Failed to start camera\")\n",
    "            return [], []\n",
    "        \n",
    "        # Initialize video writer if saving\n",
    "        video_writer = None\n",
    "        if save_video:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_writer = cv2.VideoWriter(video_path, fourcc, 20.0, (640, 480))\n",
    "            print(f\"üíæ Saving video to: {video_path}\")\n",
    "        \n",
    "        # Statistics\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "        fps_history = []\n",
    "        tracking_data = []\n",
    "        \n",
    "        print(\"üé¨ Starting detection and tracking...\")\n",
    "        print(\"Press 'q' to quit, 's' to save screenshot\")\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            # Get frame from camera\n",
    "            frame = camera_manager.get_frame()\n",
    "            if frame is None:\n",
    "                print(\"‚ö†Ô∏è No frame received from camera\")\n",
    "                break\n",
    "            \n",
    "            # Process frame\n",
    "            annotated_frame, tracked_objects, detections, fps = process_frame(\n",
    "                frame, detector, tracker\n",
    "            )\n",
    "            \n",
    "            # Collect statistics\n",
    "            frame_count += 1\n",
    "            fps_history.append(fps)\n",
    "            \n",
    "            # Store tracking data\n",
    "            for track in tracked_objects:\n",
    "                tracking_data.append({\n",
    "                    'frame': frame_count,\n",
    "                    'timestamp': time.time(),\n",
    "                    'track_id': track['id'],\n",
    "                    'class_name': track['class_name'],\n",
    "                    'confidence': track['confidence'],\n",
    "                    'bbox': track['bbox'].tolist(),\n",
    "                    'center_x': (track['bbox'][0] + track['bbox'][2]) / 2,\n",
    "                    'center_y': (track['bbox'][1] + track['bbox'][3]) / 2,\n",
    "                    'has_micro': track.get('has_micro', False),\n",
    "                    'micro_distance': track.get('micro_distance', 0)\n",
    "                })\n",
    "            \n",
    "            # Display frame (in Jupyter, we'll save frames instead)\n",
    "            if save_video and video_writer:\n",
    "                # Resize frame to match video writer dimensions\n",
    "                resized_frame = cv2.resize(annotated_frame, (640, 480))\n",
    "                video_writer.write(resized_frame)\n",
    "            \n",
    "            # Show progress\n",
    "            if frame_count % 30 == 0:  # Every 30 frames\n",
    "                avg_fps = np.mean(fps_history[-30:])\n",
    "                singers_count = len([t for t in tracked_objects if t['class_name'] == 'singer'])\n",
    "                print(f\"üìä Frame {frame_count}: {avg_fps:.1f} FPS, \"\n",
    "                      f\"{len(tracked_objects)} tracks, {singers_count} singers detected\")\n",
    "            \n",
    "            # Break if duration exceeded\n",
    "            if time.time() - start_time > duration:\n",
    "                break\n",
    "        \n",
    "        # Cleanup\n",
    "        camera_manager.release()\n",
    "        if video_writer:\n",
    "            video_writer.release()\n",
    "        \n",
    "        # Print final statistics\n",
    "        total_time = time.time() - start_time\n",
    "        avg_fps = np.mean(fps_history) if fps_history else 0\n",
    "        \n",
    "        print(f\"\\nüìà Final Statistics:\")\n",
    "        print(f\"   Total frames: {frame_count}\")\n",
    "        print(f\"   Duration: {total_time:.1f} seconds\")\n",
    "        print(f\"   Average FPS: {avg_fps:.2f}\")\n",
    "        print(f\"   Total tracking records: {len(tracking_data)}\")\n",
    "        \n",
    "        return tracking_data, fps_history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during camera processing: {e}\")\n",
    "        camera_manager.release()\n",
    "        if video_writer:\n",
    "            video_writer.release()\n",
    "        return [], []\n",
    "\n",
    "def simulate_camera_demo():\n",
    "    \"\"\"Simulate camera demo with realistic person/microphone scenarios\"\"\"\n",
    "    print(\"üé≠ Simulating camera demo with singer detection...\")\n",
    "    \n",
    "    # Create sample frames with simulated detections\n",
    "    tracking_data = []\n",
    "    fps_list = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        # Simulate detections for each frame\n",
    "        fake_detections = []\n",
    "        \n",
    "        # Person 1 - moves across screen, gets microphone in middle frames\n",
    "        person1_x = 50 + i * 15\n",
    "        person1_bbox = np.array([person1_x, 150, person1_x + 60, 300])\n",
    "        fake_detections.append({\n",
    "            'bbox': person1_bbox,\n",
    "            'confidence': 0.85,\n",
    "            'class_name': 'person',\n",
    "            'class_id': 0\n",
    "        })\n",
    "        \n",
    "        # Microphone appears near person 1 in frames 8-15 (simulating singing)\n",
    "        if 8 <= i <= 15:\n",
    "            micro1_x = person1_x + 30\n",
    "            micro1_bbox = np.array([micro1_x, 180, micro1_x + 20, 220])\n",
    "            fake_detections.append({\n",
    "                'bbox': micro1_bbox,\n",
    "                'confidence': 0.90,\n",
    "                'class_name': 'micro',\n",
    "                'class_id': 1\n",
    "            })\n",
    "        \n",
    "        # Person 2 - stationary, no microphone\n",
    "        if i >= 5:  # appears later in the sequence\n",
    "            person2_bbox = np.array([400, 100, 460, 280])\n",
    "            fake_detections.append({\n",
    "                'bbox': person2_bbox,\n",
    "                'confidence': 0.75,\n",
    "                'class_name': 'person',\n",
    "                'class_id': 0\n",
    "            })\n",
    "        \n",
    "        # Standalone microphone (not near anyone)\n",
    "        if i >= 10:\n",
    "            standalone_micro_bbox = np.array([500, 350, 520, 390])\n",
    "            fake_detections.append({\n",
    "                'bbox': standalone_micro_bbox,\n",
    "                'confidence': 0.80,\n",
    "                'class_name': 'micro',\n",
    "                'class_id': 1\n",
    "            })\n",
    "        \n",
    "        # Update tracker with fake detections\n",
    "        tracked_objects = tracker.update(fake_detections)\n",
    "        \n",
    "        # Simulate FPS\n",
    "        fps_list.append(25.0 + np.random.uniform(-5, 5))\n",
    "        \n",
    "        # Store tracking data\n",
    "        for track in tracked_objects:\n",
    "            tracking_data.append({\n",
    "                'frame': i,\n",
    "                'track_id': track['id'],\n",
    "                'class_name': track['class_name'],\n",
    "                'confidence': track['confidence'],\n",
    "                'center_x': (track['bbox'][0] + track['bbox'][2]) / 2,\n",
    "                'center_y': (track['bbox'][1] + track['bbox'][3]) / 2,\n",
    "                'has_micro': track.get('has_micro', False),\n",
    "                'micro_distance': track.get('micro_distance', 0)\n",
    "            })\n",
    "    \n",
    "    return tracking_data, fps_list\n",
    "\n",
    "# Run camera demo or simulation\n",
    "if RUN_CAMERA_DEMO:\n",
    "    print(\"üöÄ Starting real camera demo...\")\n",
    "    print(f\"‚è±Ô∏è Duration: {DEMO_DURATION} seconds\")\n",
    "    \n",
    "    # Make sure results directory exists\n",
    "    Path(\"results\").mkdir(exist_ok=True)\n",
    "    \n",
    "    tracking_data, fps_history = run_camera_detection(\n",
    "        duration=DEMO_DURATION,\n",
    "        save_video=SAVE_VIDEO,\n",
    "        video_path=\"results/detection_output.mp4\"\n",
    "    )\n",
    "else:\n",
    "    print(\"üîÑ Running simulation demo (set RUN_CAMERA_DEMO=True for real camera)\")\n",
    "    tracking_data, fps_history = simulate_camera_demo()\n",
    "\n",
    "print(f\"‚úÖ Demo completed! Collected {len(tracking_data)} tracking records\")\n",
    "\n",
    "# Show singer detection summary\n",
    "if tracking_data:\n",
    "    df = pd.DataFrame(tracking_data)\n",
    "    if 'class_name' in df.columns:\n",
    "        class_summary = df['class_name'].value_counts()\n",
    "        print(f\"\\nüìä Object Detection Summary:\")\n",
    "        for class_name, count in class_summary.items():\n",
    "            print(f\"   {class_name}: {count} detections\")\n",
    "        \n",
    "        # Singer-specific summary\n",
    "        singer_data = df[df['class_name'] == 'singer']\n",
    "        if len(singer_data) > 0:\n",
    "            print(f\"\\nüé§ Singer Detection Details:\")\n",
    "            print(f\"   Singer tracks: {singer_data['track_id'].nunique()}\")\n",
    "            print(f\"   Total singer detections: {len(singer_data)}\")\n",
    "            if 'micro_distance' in singer_data.columns:\n",
    "                avg_distance = singer_data[singer_data['micro_distance'] > 0]['micro_distance'].mean()\n",
    "                print(f\"   Average microphone distance: {avg_distance:.1f} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0a111",
   "metadata": {},
   "source": [
    "## 10. Display Results with Bounding Boxes and Tracking IDs\n",
    "\n",
    "Visualize the detection and tracking results with detailed annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and analysis of results\n",
    "def analyze_tracking_results(tracking_data):\n",
    "    \"\"\"Analyze tracking results and create visualizations\"\"\"\n",
    "    \n",
    "    if not tracking_data:\n",
    "        print(\"‚ö†Ô∏è No tracking data available\")\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(tracking_data)\n",
    "    \n",
    "    print(f\"üìä Tracking Analysis:\")\n",
    "    print(f\"   Total tracking records: {len(df)}\")\n",
    "    print(f\"   Unique objects tracked: {df['track_id'].nunique()}\")\n",
    "    print(f\"   Frames processed: {df['frame'].nunique()}\")\n",
    "    print(f\"   Object classes detected: {df['class_name'].unique()}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = df['class_name'].value_counts()\n",
    "    print(f\"\\nüè∑Ô∏è Class Distribution:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"   {class_name}: {count} detections\")\n",
    "    \n",
    "    # Track duration analysis\n",
    "    track_durations = df.groupby('track_id')['frame'].agg(['min', 'max', 'count'])\n",
    "    track_durations['duration'] = track_durations['max'] - track_durations['min'] + 1\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Track Duration Statistics:\")\n",
    "    print(f\"   Average track duration: {track_durations['duration'].mean():.1f} frames\")\n",
    "    print(f\"   Longest track: {track_durations['duration'].max()} frames\")\n",
    "    print(f\"   Shortest track: {track_durations['duration'].min()} frames\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Class distribution pie chart\n",
    "    if len(class_counts) > 0:\n",
    "        axes[0, 0].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%')\n",
    "        axes[0, 0].set_title('Object Class Distribution')\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'No data available', ha='center', va='center')\n",
    "        axes[0, 0].set_title('Object Class Distribution')\n",
    "    \n",
    "    # 2. Track duration histogram\n",
    "    if len(track_durations) > 0:\n",
    "        axes[0, 1].hist(track_durations['duration'], bins=10, edgecolor='black')\n",
    "        axes[0, 1].set_title('Track Duration Distribution')\n",
    "        axes[0, 1].set_xlabel('Duration (frames)')\n",
    "        axes[0, 1].set_ylabel('Number of tracks')\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No data available', ha='center', va='center')\n",
    "        axes[0, 1].set_title('Track Duration Distribution')\n",
    "    \n",
    "    # 3. Detections per frame\n",
    "    if len(df) > 0:\n",
    "        detections_per_frame = df.groupby('frame').size()\n",
    "        axes[1, 0].plot(detections_per_frame.index, detections_per_frame.values)\n",
    "        axes[1, 0].set_title('Detections per Frame')\n",
    "        axes[1, 0].set_xlabel('Frame')\n",
    "        axes[1, 0].set_ylabel('Number of detections')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No data available', ha='center', va='center')\n",
    "        axes[1, 0].set_title('Detections per Frame')\n",
    "    \n",
    "    # 4. Confidence distribution\n",
    "    if 'confidence' in df.columns and len(df) > 0:\n",
    "        axes[1, 1].hist(df['confidence'], bins=20, edgecolor='black')\n",
    "        axes[1, 1].set_title('Confidence Score Distribution')\n",
    "        axes[1, 1].set_xlabel('Confidence')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No confidence data', ha='center', va='center')\n",
    "        axes[1, 1].set_title('Confidence Score Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_tracking_trajectory_plot(tracking_data):\n",
    "    \"\"\"Create trajectory plot showing object movements\"\"\"\n",
    "    \n",
    "    if not tracking_data:\n",
    "        print(\"‚ö†Ô∏è No tracking data for trajectory plot\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(tracking_data)\n",
    "    \n",
    "    if 'center_x' not in df.columns or 'center_y' not in df.columns:\n",
    "        print(\"‚ö†Ô∏è No position data available for trajectory plot\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot trajectory for each track\n",
    "    unique_tracks = df['track_id'].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_tracks)))\n",
    "    \n",
    "    for i, track_id in enumerate(unique_tracks):\n",
    "        track_data = df[df['track_id'] == track_id]\n",
    "        \n",
    "        if len(track_data) > 1:\n",
    "            plt.plot(track_data['center_x'], track_data['center_y'], \n",
    "                    color=colors[i], linewidth=2, alpha=0.7, \n",
    "                    label=f'Track {track_id}')\n",
    "            \n",
    "            # Mark start and end points\n",
    "            plt.scatter(track_data['center_x'].iloc[0], track_data['center_y'].iloc[0],\n",
    "                       color=colors[i], s=100, marker='o', edgecolor='black')\n",
    "            plt.scatter(track_data['center_x'].iloc[-1], track_data['center_y'].iloc[-1],\n",
    "                       color=colors[i], s=100, marker='s', edgecolor='black')\n",
    "    \n",
    "    plt.title('Object Tracking Trajectories')\n",
    "    plt.xlabel('X Position (pixels)')\n",
    "    plt.ylabel('Y Position (pixels)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().invert_yaxis()  # Invert Y axis to match image coordinates\n",
    "    \n",
    "    # Add legend for markers\n",
    "    plt.scatter([], [], c='gray', s=100, marker='o', \n",
    "               edgecolor='black', label='Start')\n",
    "    plt.scatter([], [], c='gray', s=100, marker='s', \n",
    "               edgecolor='black', label='End')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance_metrics(fps_history):\n",
    "    \"\"\"Plot performance metrics\"\"\"\n",
    "    \n",
    "    if not fps_history:\n",
    "        print(\"‚ö†Ô∏è No FPS data available\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fps_history)\n",
    "    plt.title('Real-time Performance (FPS)')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('FPS')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(fps_history, bins=20, edgecolor='black')\n",
    "    plt.title('FPS Distribution')\n",
    "    plt.xlabel('FPS')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(np.mean(fps_history), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(fps_history):.1f} FPS')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìà Performance Summary:\")\n",
    "    print(f\"   Average FPS: {np.mean(fps_history):.2f}\")\n",
    "    print(f\"   Min FPS: {np.min(fps_history):.2f}\")\n",
    "    print(f\"   Max FPS: {np.max(fps_history):.2f}\")\n",
    "    print(f\"   FPS Std Dev: {np.std(fps_history):.2f}\")\n",
    "\n",
    "# Analyze results\n",
    "print(\"üìä Analyzing tracking results...\")\n",
    "df_results = analyze_tracking_results(tracking_data)\n",
    "\n",
    "print(\"\\nüõ§Ô∏è Creating trajectory visualization...\")\n",
    "create_tracking_trajectory_plot(tracking_data)\n",
    "\n",
    "print(\"\\nüìà Performance analysis...\")\n",
    "plot_performance_metrics(fps_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659be8b",
   "metadata": {},
   "source": [
    "## 11. Save Tracking Results\n",
    "\n",
    "Export tracking data and results for further analysis and record keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14936d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tracking_results(tracking_data, fps_history, output_dir=\"results\"):   \n",
    "    \"\"\"Save all tracking results and analysis\"\"\"   \n",
    "    output_path = Path(output_dir)    \n",
    "    output_path.mkdir(exist_ok=True)       \n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save tracking data as CSV\n",
    "    if tracking_data:        \n",
    "        df = pd.DataFrame(tracking_data)\n",
    "        csv_path = output_path / f\"tracking_data_{timestamp}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"üíæ Tracking data saved to: {csv_path}\")\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary_path = output_path / f\"tracking_summary_{timestamp}.txt\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(f\"YOLO 11 Object Detection and Tracking Results\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"=\"*50 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Dataset Statistics:\\n\")\n",
    "            f.write(f\"  Total tracking records: {len(df)}\\n\")\n",
    "            f.write(f\"  Unique objects tracked: {df['track_id'].nunique()}\\n\")\n",
    "            f.write(f\"  Frames processed: {df['frame'].nunique()}\\n\")\n",
    "            f.write(f\"  Classes detected: {', '.join(df['class_name'].unique())}\\n\")\n",
    "            \n",
    "            if 'confidence' in df.columns:\n",
    "                f.write(f\"\\nConfidence Statistics:\\n\")\n",
    "                f.write(f\"  Average confidence: {df['confidence'].mean():.3f}\\n\")\n",
    "                f.write(f\"  Min confidence: {df['confidence'].min():.3f}\\n\")\n",
    "                f.write(f\"  Max confidence: {df['confidence'].max():.3f}\\n\")\n",
    "            \n",
    "            # Track duration analysis\n",
    "            track_durations = df.groupby('track_id')['frame'].agg(['min', 'max', 'count'])\n",
    "            track_durations['duration'] = track_durations['max'] - track_durations['min'] + 1\n",
    "            \n",
    "            f.write(f\"\\nTracking Performance:\\n\")\n",
    "            f.write(f\"  Average track duration: {track_durations['duration'].mean():.1f} frames\\n\")\n",
    "            f.write(f\"  Longest track: {track_durations['duration'].max()} frames\\n\")\n",
    "            f.write(f\"  Shortest track: {track_durations['duration'].min()} frames\\n\")\n",
    "            \n",
    "        print(f\"üìã Summary report saved to: {summary_path}\")\n",
    "    else:\n",
    "        csv_path = None\n",
    "        summary_path = None\n",
    "    \n",
    "    # Save FPS data\n",
    "    if fps_history:\n",
    "        fps_df = pd.DataFrame({\n",
    "            'frame': range(len(fps_history)),\n",
    "            'fps': fps_history\n",
    "        })\n",
    "        fps_path = output_path / f\"fps_data_{timestamp}.csv\"\n",
    "        fps_df.to_csv(fps_path, index=False)\n",
    "        print(f\"‚ö° FPS data saved to: {fps_path}\")\n",
    "    else:\n",
    "        fps_path = None\n",
    "    \n",
    "    # Save configuration snapshot\n",
    "    config_snapshot = {\n",
    "        'model': {\n",
    "            'name': MODEL_SIZE,\n",
    "            'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "            'iou_threshold': IOU_THRESHOLD\n",
    "        },\n",
    "        'tracking': {\n",
    "            'max_age': MAX_TRACK_AGE,\n",
    "            'min_hits': MIN_TRACK_HITS,\n",
    "            'iou_threshold': IOU_THRESHOLD\n",
    "        },\n",
    "        'processing': {\n",
    "            'timestamp': timestamp,\n",
    "            'total_frames': len(fps_history) if fps_history else 0,\n",
    "            'avg_fps': np.mean(fps_history) if fps_history else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = output_path / f\"config_snapshot_{timestamp}.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_snapshot, f, indent=2)\n",
    "    print(f\"‚öôÔ∏è Configuration snapshot saved to: {config_path}\")\n",
    "    \n",
    "    return {\n",
    "        'tracking_data_path': csv_path if tracking_data else None,\n",
    "        'summary_path': summary_path if tracking_data else None,\n",
    "        'fps_data_path': fps_path if fps_history else None,\n",
    "        'config_path': config_path\n",
    "    }\n",
    "\n",
    "def export_to_label_studio_format(tracking_data, output_path=\"results/labelstudio_export.json\"):\n",
    "    \"\"\"Export tracking results in Label Studio import format\"\"\"\n",
    "    \n",
    "    if not tracking_data:\n",
    "        print(\"‚ö†Ô∏è No tracking data to export\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(tracking_data)\n",
    "    \n",
    "    # Group by frame\n",
    "    label_studio_tasks = []\n",
    "    \n",
    "    for frame_num in df['frame'].unique():\n",
    "        frame_data = df[df['frame'] == frame_num]\n",
    "        \n",
    "        # Create task for this frame\n",
    "        task = {\n",
    "            \"data\": {\n",
    "                \"image\": f\"frame_{frame_num:06d}.jpg\"  # Placeholder image name\n",
    "            },\n",
    "            \"predictions\": [{\n",
    "                \"model_version\": \"yolo11\",\n",
    "                \"result\": []\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        # Add annotations for each detection in this frame\n",
    "        for _, row in frame_data.iterrows():\n",
    "            if 'bbox' in row and isinstance(row['bbox'], list):\n",
    "                x1, y1, x2, y2 = row['bbox']\n",
    "                \n",
    "                # Convert to Label Studio format (percentages)\n",
    "                # Assuming frame size of 640x480 (you may need to adjust)\n",
    "                frame_width, frame_height = 640, 480\n",
    "                \n",
    "                x_percent = (x1 / frame_width) * 100\n",
    "                y_percent = (y1 / frame_height) * 100\n",
    "                width_percent = ((x2 - x1) / frame_width) * 100\n",
    "                height_percent = ((y2 - y1) / frame_height) * 100\n",
    "                \n",
    "                annotation = {\n",
    "                    \"from_name\": \"label\",\n",
    "                    \"to_name\": \"image\",\n",
    "                    \"type\": \"rectanglelabels\",\n",
    "                    \"value\": {\n",
    "                        \"x\": x_percent,\n",
    "                        \"y\": y_percent,\n",
    "                        \"width\": width_percent,\n",
    "                        \"height\": height_percent,\n",
    "                        \"rectanglelabels\": [row['class_name']]\n",
    "                    },\n",
    "                    \"score\": float(row.get('confidence', 1.0))\n",
    "                }\n",
    "                \n",
    "                task[\"predictions\"][0][\"result\"].append(annotation)\n",
    "        \n",
    "        label_studio_tasks.append(task)\n",
    "    \n",
    "    # Save to JSON file\n",
    "    output_file = Path(output_path)\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(label_studio_tasks, f, indent=2)\n",
    "    \n",
    "    print(f\"üì§ Label Studio export saved to: {output_file}\")\n",
    "    print(f\"   Total tasks: {len(label_studio_tasks)}\")\n",
    "    \n",
    "    return str(output_file)\n",
    "\n",
    "# Save all results\n",
    "print(\"üíæ Saving tracking results and analysis...\")\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "\n",
    "saved_files = save_tracking_results(tracking_data, fps_history)\n",
    "\n",
    "print(\"\\nüì§ Exporting to Label Studio format...\")\n",
    "labelstudio_export_path = export_to_label_studio_format(tracking_data)\n",
    "\n",
    "print(\"\\n‚úÖ All results saved successfully!\")\n",
    "print(\"\\nüìÅ Generated Files:\")\n",
    "for key, path in saved_files.items():\n",
    "    if path:\n",
    "        print(f\"   {key}: {path}\")\n",
    "        \n",
    "if labelstudio_export_path:\n",
    "    print(f\"   labelstudio_export: {labelstudio_export_path}\")\n",
    "\n",
    "print(\"\\nüéâ Tutorial completed successfully!\")\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(\"   1. Run with real camera data (set RUN_CAMERA_DEMO=True)\")\n",
    "print(\"   2. Train custom models with your labeled data\")\n",
    "print(\"   3. Integrate with Label Studio for continuous improvement\")\n",
    "print(\"   4. Optimize performance for your specific use case\")\n",
    "print(\"   5. Deploy as a service using the provided API code\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
